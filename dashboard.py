import streamlit as st
import torch
import pandas as pd
import numpy as np
import os
import sys
import json
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px
from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score

try:
    from scipy.interpolate import interp1d
except ImportError:
    st.error("FATAL ERROR: 'scipy' is not installed. Please run 'pip install scipy' or add it to your requirements.txt")
    st.stop()


# --- Add src directory to path ---
# This allows importing 'model.py'
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), 'src')))
try:
    from model import TimeSeriesTransformer
except ImportError:
    st.error("FATAL ERROR: Could not find 'src/model.py'. Make sure this dashboard is run from the project's root directory.")
    st.stop()
except ModuleNotFoundError as e:
    st.error(f"FATAL ERROR: A core library is not installed: {e}")
    st.info("Please activate your project's virtual environment (`venv\\Scripts\\activate`) and run `pip install -r requirements.txt`")
    st.stop()

# --- App Constants ---
MODEL_PATH = "server_out/global_best.pt"
N_FEATURES = 40
SEQ_LEN = 48

# Paths to the evaluation files (THESE MUST BE GENERATED FIRST)
EVAL_FEDERATED_JSON = "eval_results_federated.json"
EVAL_LOCAL_JSON = "eval_results_local.json"

# Paths to the plot files (generated by plot_results.py)
PLOT_ROC_PATH = "model_comparison_plot.png"
# Infer PRC path from ROC path
base, ext = os.path.splitext(PLOT_ROC_PATH)
PLOT_PRC_PATH = f"{base}_prc{ext}"

# --- Page Configuration ---
st.set_page_config(
    page_title="Neonatal Sepsis Prediction",
    page_icon="üë∂",
    layout="wide"
)

# --- Model & Data Loading (Cached) ---
@st.cache_resource
def load_model(model_path, n_features, seq_len):
    """Loads the trained PyTorch model."""
    if not os.path.exists(model_path):
        return None  # Will be handled in main app logic
    model = TimeSeriesTransformer(n_features=n_features, seq_len=seq_len)
    try:
        # Load the state_dict, ensuring it runs on CPU
        model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))
    except RuntimeError:
        st.error(f"Error loading model weights from {model_path}. File may be corrupt or not match the architecture (Features={n_features}, SeqLen={seq_len}).")
        st.stop()
    model.eval()
    return model

@st.cache_data
def load_eval_data(json_path):
    """Loads y_true and y_prob from the evaluation JSON file."""
    if not os.path.exists(json_path):
        return None
    try:
        with open(json_path, 'r') as f:
            data = json.load(f)
        return data
    except Exception:
        return None

# --- Data Processing Function ---
@st.cache_data
def preprocess_data(df, seq_len, n_features):
    """Validates and preprocesses the uploaded DataFrame."""
    try:
        df_numeric = df.apply(pd.to_numeric, errors='coerce')
    except Exception as e:
        return None, f"Error converting data to numeric: {e}"

    if df_numeric.isnull().values.any():
        st.warning("Non-numeric values detected in CSV. They will be treated as 0.")
        df_numeric = df_numeric.fillna(0)

    if df_numeric.shape[1] != n_features:
        return None, f"Invalid CSV: Expected {n_features} columns, but got {df_numeric.shape[1]}."

    current_rows = df_numeric.shape[0]
    if current_rows > seq_len:
        st.warning(f"Data has {current_rows} rows. Truncating to the **last {seq_len} rows** (most recent timesteps).")
        df_processed = df_numeric.tail(seq_len)
    elif current_rows < seq_len:
        st.warning(f"Data has {current_rows} rows. Padding with {seq_len - current_rows} rows of zeros at the **beginning**.")
        padding = pd.DataFrame(np.zeros((seq_len - current_rows, n_features)), columns=df_numeric.columns)
        df_processed = pd.concat([padding, df_numeric], ignore_index=True)
    else:
        df_processed = df_numeric
    
    data_np = df_processed.to_numpy(dtype=np.float32)
    data_tensor = torch.tensor(data_np).unsqueeze(0)  # Add batch dimension -> (1, 48, 40)
    return data_tensor, "Data processed successfully."

# --- Prediction Function ---
def make_prediction(model, tensor_data):
    """Runs the model and returns the sepsis risk probability."""
    with torch.no_grad():
        logits = model(tensor_data) # Use the original 1D-output model
        probability = torch.sigmoid(logits).item()
    return probability

# --- Plotting Functions ---

@st.cache_data
def calculate_all_metrics(eval_data, threshold=0.5):
    """Calculates all metrics from the eval data dict."""
    y_true = np.array(eval_data['y_true'])
    y_prob = np.array(eval_data['y_prob'])
    y_pred = (y_prob > threshold).astype(int)
    
    return {
        "Model": eval_data.get('model_name', 'Model').replace('_', ' ').replace('Eval Results ', '').title(),
        "AUROC": eval_data.get('auroc', np.nan),
        "AUPRC": eval_data.get('auprc', np.nan),
        "Accuracy": accuracy_score(y_true, y_pred),
        "F1-Score": f1_score(y_true, y_pred),
        "Precision": precision_score(y_true, y_pred, zero_division=0),
        "Recall": recall_score(y_true, y_pred, zero_division=0)
    }

def plot_metric_bars(df_metrics, metric_name, color_scale):
    """Generates a Plotly bar chart for a given metric."""
    fig = px.bar(df_metrics, 
                 x='Model', 
                 y=metric_name,
                 title=f'{metric_name} Comparison',
                 labels={'Model': 'Model', metric_name: metric_name},
                 color=metric_name,
                 color_continuous_scale=color_scale,
                 text_auto='.3f')
    fig.update_layout(height=400, showlegend=False)
    return fig

@st.cache_data
def plot_prediction_histogram(probs_neg, probs_pos, prediction_score):
    """Plots the prediction score on a histogram of the test set scores."""
    fig, ax = plt.subplots(figsize=(10, 5))
    ax.hist(probs_neg, bins=50, alpha=0.7, label='Actual Low-Risk (Test Set)', color='blue', density=True)
    ax.hist(probs_pos, bins=50, alpha=0.7, label='Actual High-Risk (Test Set)', color='orange', density=True)
    ax.axvline(x=prediction_score, color='red', linestyle='--', lw=3, label=f'Your Prediction ({prediction_score:.2f})')
    ax.set_title('Prediction Score vs. Test Set Outcomes', fontsize=14)
    ax.set_xlabel('Predicted Risk Score (Probability)', fontsize=12)
    ax.set_ylabel('Density of Patients', fontsize=12)
    ax.legend(loc='upper right')
    ax.grid(True, linestyle='--', alpha=0.5)
    return fig

def plot_confusion_matrix(ax, y_true, y_prob, title, threshold=0.5):
    """Plots a confusion matrix on a given matplotlib axis."""
    y_pred = (np.array(y_prob) > threshold).astype(int)
    cm = confusion_matrix(y_true, y_pred)
    group_names = ['True Negative', 'False Positive', 'False Negative', 'True Positive']
    group_counts = [f"{value}" for value in cm.flatten()]
    group_percentages = [f"{value:.2%}" for value in cm.flatten() / np.sum(cm)]
    labels = [f"{v1}\n{v2}\n{v3}" for v1, v2, v3 in zip(group_names, group_counts, group_percentages)]
    labels = np.asarray(labels).reshape(2, 2)
    
    sns.heatmap(cm, annot=labels, fmt='', cmap='Blues', ax=ax, cbar=False)
    ax.set_title(title, fontsize=16)
    ax.set_xlabel('Predicted Label', fontsize=12)
    ax.set_ylabel('True Label', fontsize=12)
    
# --- Main App ---
def main():
    st.title("üë∂ Neonatal Sepsis Prediction Dashboard")

    # --- Load all resources ---
    model = load_model(MODEL_PATH, N_FEATURES, SEQ_LEN)
    eval_data_fed = load_eval_data(EVAL_FEDERATED_JSON)
    eval_data_local = load_eval_data(EVAL_LOCAL_JSON)

    if model is None:
        st.error(f"FATAL ERROR: Model file not found at '{MODEL_PATH}'.")
        st.info("Please run the federated learning simulation first (`python src/fl_server.py ...`) to generate this file.")
        st.stop()

    # --- Sidebar Navigation ---
    st.sidebar.title("üóÇÔ∏è Navigation")
    page = st.sidebar.radio("Select Page", ["Live Prediction", "Model Performance"])

    # ==============================================================================
    # PAGE 1: LIVE PREDICTION
    # ==============================================================================
    if page == "Live Prediction":
        st.header("üè• Sepsis Risk Prediction")
        
        st.info(f"Upload a patient's time-series data as a CSV file. The file must have **{N_FEATURES} columns** (features) with no header. The dashboard will automatically handle any number of rows by padding or truncating to the required **{SEQ_LEN} timesteps**.")
        
        @st.cache_data
        def get_template_csv():
            template_df = pd.DataFrame(np.round(np.random.rand(SEQ_LEN, N_FEATURES), 2))
            return template_df.to_csv(header=False, index=False).encode('utf-8')

        st.download_button(
            label=f"Download Template CSV ({SEQ_LEN} rows x {N_FEATURES} cols)",
            data=get_template_csv(),
            file_name="patient_template.csv",
            mime="text/csv",
        )
        
        uploaded_file = st.file_uploader("Upload patient CSV", type="csv")
        
        if uploaded_file is not None:
            try:
                df = pd.read_csv(uploaded_file, header=None)
                with st.expander("Show Uploaded Data (first 5 rows)"):
                    st.dataframe(df.head())
                
                tensor_data, message = preprocess_data(df, SEQ_LEN, N_FEATURES)
                
                if tensor_data is not None:
                    if st.button("üîç Predict Sepsis Risk", use_container_width=True):
                        with st.spinner("Running model..."):
                            probability = make_prediction(model, tensor_data)
                            
                            st.subheader("Prediction Result")
                            col1, col2 = st.columns(2)
                            
                            with col1:
                                # Plotly Gauge
                                fig_gauge = go.Figure(go.Indicator(
                                    mode="gauge+number",
                                    value=probability * 100,
                                    domain={'x': [0, 1], 'y': [0, 1]},
                                    title={'text': "Sepsis Risk", 'font': {'size': 24}},
                                    gauge={
                                        'axis': {'range': [None, 100], 'tickwidth': 1, 'tickcolor': "darkblue"},
                                        'bar': {'color': "darkblue"},
                                        'bgcolor': "white",
                                        'borderwidth': 2,
                                        'bordercolor': "gray",
                                        'steps': [
                                            {'range': [0, 25], 'color': 'rgba(44, 160, 44, 0.7)'},  # Green
                                            {'range': [25, 50], 'color': 'rgba(255, 127, 14, 0.7)'}, # Orange
                                            {'range': [50, 100], 'color': 'rgba(214, 39, 40, 0.7)'}  # Red
                                        ],
                                        'threshold': {
                                            'line': {'color': "red", 'width': 4},
                                            'thickness': 0.75,
                                            'value': 50 # Default 0.5 threshold line
                                        }
                                    }
                                ))
                                fig_gauge.update_layout(height=300, margin=dict(t=50, b=50))
                                st.plotly_chart(fig_gauge, use_container_width=True)

                            with col2:
                                # Clinical Recommendations
                                if probability > 0.5:
                                    st.error("HIGH RISK: Sepsis likely.", icon="‚ö†Ô∏è")
                                    st.markdown("""
                                    **Clinical Recommendations:**
                                    - Immediate medical evaluation by attending physician.
                                    - Consider broad-spectrum antibiotics.
                                    - Send blood cultures and inflammatory markers (CRP, PCT).
                                    - Close monitoring of vital signs.
                                    """)
                                elif probability > 0.25:
                                    st.warning("Moderate Risk: Continue close monitoring.", icon="‚ÑπÔ∏è")
                                    st.markdown("""
                                    **Clinical Recommendations:**
                                    - Increase monitoring frequency.
                                    - Be vigilant for any clinical deterioration.
                                    - Low threshold to draw labs if any new concerns arise.
                                    """)
                                else:
                                    st.success("Low Risk: Sepsis unlikely.", icon="‚úÖ")
                                    st.markdown("""
                                    **Clinical Recommendations:**
                                    - Continue standard monitoring.
                                    - Follow routine hospital protocols.
                                    """)
                            
                            # --- Prediction Analysis Plot ---
                            st.subheader("Prediction Analysis")
                            st.markdown("This plot shows where your new prediction (red line) falls in relation to the test set scores.")
                            
                            if eval_data_fed:
                                probs_neg = np.array(eval_data_fed['y_prob'])[np.array(eval_data_fed['y_true']) == 0]
                                probs_pos = np.array(eval_data_fed['y_prob'])[np.array(eval_data_fed['y_true']) == 1]
                                fig = plot_prediction_histogram(probs_neg, probs_pos, probability)
                                st.pyplot(fig)
                            else:
                                st.info(f"Cannot display analysis plot because the test set evaluation file '{EVAL_FEDERATED_JSON}' is missing.")
                
            except Exception as e:
                st.error(f"An error occurred while processing the file: {e}")

    # ==============================================================================
    # PAGE 2: MODEL PERFORMANCE
    # ==============================================================================
    else: 
        st.header("üìä Model Performance Evaluation")
        st.markdown("This page compares our **Federated Model** against a **Local Model** (trained on a single client) to demonstrate the power of federated learning.")

        # --- Load Data and Calculate Metrics ---
        if not eval_data_fed or not eval_data_local:
            st.error(f"Could not load evaluation data from '{EVAL_FEDERATED_JSON}' or '{EVAL_LOCAL_JSON}'.")
            st.info("Please run `python src/evaluate.py ...` for both local and federated models to generate these files.")
            st.stop()
        
        metrics_fed = calculate_all_metrics(eval_data_fed)
        metrics_local = calculate_all_metrics(eval_data_local)
        
        df_metrics = pd.DataFrame([metrics_fed, metrics_local])
        df_metrics_plot = df_metrics.set_index('Model') # For table
        df_metrics_bar = df_metrics.melt('Model', var_name='Metric', value_name='Score')

        # --- Create Tabs ---
        tab_list = [
            "üìä Metric Summary",
            "üìà ROC & PRC Curves",
            "üî¢ Confusion Matrices"
        ]
        tab1, tab2, tab3 = st.tabs(tab_list)

        with tab1:
            st.subheader("Metric Summary Table")
            st.dataframe(df_metrics_plot.style.format("{:.3f}"), use_container_width=True)
            
            st.subheader("Key Metric Comparison")
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Best AUROC", f"{df_metrics_plot['AUROC'].max():.3f}", f"{df_metrics_plot['AUROC'].idxmax()}")
            with col2:
                st.metric("Best F1-Score", f"{df_metrics_plot['F1-Score'].max():.3f}", f"{df_metrics_plot['F1-Score'].idxmax()}")
            with col3:
                st.metric("Best Recall", f"{df_metrics_plot['Recall'].max():.3f}", f"{df_metrics_plot['Recall'].idxmax()}")

            st.subheader("Performance Bar Charts")
            c1, c2 = st.columns(2)
            with c1:
                st.plotly_chart(plot_metric_bars(df_metrics, "AUROC", "Blues"), use_container_width=True)
                st.plotly_chart(plot_metric_bars(df_metrics, "F1-Score", "Greens"), use_container_width=True)
                st.plotly_chart(plot_metric_bars(df_metrics, "Recall", "Oranges"), use_container_width=True)
            with c2:
                st.plotly_chart(plot_metric_bars(df_metrics, "AUPRC", "Reds"), use_container_width=True)
                st.plotly_chart(plot_metric_bars(df_metrics, "Precision", "Purples"), use_container_width=True)
                st.plotly_chart(plot_metric_bars(df_metrics, "Accuracy", "Greys"), use_container_width=True)
                
            st.subheader("Normalized Metric Profile (Radar Chart)")
            
            # Normalize data for radar plot
            df_norm = df_metrics_plot.copy()
            for col in df_norm.columns:
                if (df_norm[col].max() - df_norm[col].min()) != 0:
                    df_norm[col] = (df_norm[col] - df_norm[col].min()) / (df_norm[col].max() - df_norm[col].min())
                else:
                    df_norm[col] = 0.5 # Avoid division by zero
                    
            fig_radar = go.Figure()
            for idx, row in df_norm.iterrows():
                fig_radar.add_trace(go.Scatterpolar(
                    r=row.values.tolist() + [row.values[0]], # Close the loop
                    theta=row.index.tolist() + [row.index[0]], # Close the loop
                    fill='toself',
                    name=idx
                ))
            fig_radar.update_layout(
                polar=dict(radialaxis=dict(visible=True, range=[0, 1])),
                showlegend=True,
                height=500,
                title='Normalized Metrics Profile'
            )
            st.plotly_chart(fig_radar, use_container_width=True)


        with tab2:
            st.subheader("ROC & Precision-Recall (PRC) Curves")
            st.markdown("These plots (generated by `plot_results.py`) show the model's ability to discriminate between high-risk and low-risk patients on unseen test data.")
            
            col1, col2 = st.columns(2)
            with col1:
                if os.path.exists(PLOT_ROC_PATH):
                    st.image(PLOT_ROC_PATH, caption="ROC Curve: Higher and to the left is better.")
                    st.markdown("""
                    **Interpretation:**
                    * **AUROC** measures the model's ability to distinguish between "septic" and "non-septic" patients.
                    * The **shaded bands** show the 95% confidence interval. The lack of overlap proves our federated model's superiority is statistically significant.
                    """)
                else:
                    st.warning(f"Plot not found: '{PLOT_ROC_PATH}'. Please run `python src/plot_results.py ...` first.")

            with col2:
                # Infer PRC path from ROC path
                base, ext = os.path.splitext(PLOT_ROC_PATH)
                prc_plot_path = f"{base}_prc{ext}"
                if os.path.exists(prc_plot_path):
                    st.image(prc_plot_path, caption="PRC Curve: Higher and to the right is better.")
                    st.markdown("""
                    **Interpretation:**
                    * **AUPRC** is a better metric for imbalanced data (like sepsis). It answers: "Of all the high-risk alerts, what percentage are correct?"
                    * The **"Chance" line (dotted black)** shows the baseline. Our federated model is far superior.
                    """)
                else:
                    st.warning(f"Plot not found: '{prc_plot_path}'. Please run `python src/plot_results.py ...` first.")
                    
        with tab3:
            st.subheader("Confusion Matrices (Federated vs. Local)")
            st.markdown("This shows the *actual* count of correct and incorrect predictions on the test set (using a 0.5 risk threshold).")

            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
            
            # Plot Federated CM
            plot_confusion_matrix(ax1, eval_data_fed['y_true'], eval_data_fed['y_prob'], 
                                  title=f"Federated Model ({eval_data_fed.get('model_name', 'Federated')})")
            
            # Plot Local CM
            plot_confusion_matrix(ax2, eval_data_local['y_true'], eval_data_local['y_prob'], 
                                  title=f"Local Model ({eval_data_local.get('model_name', 'Local')})")
            
            plt.tight_layout()
            st.pyplot(fig)

if __name__ == "__main__":
    main()
